const { initializeApp } = require('firebase/app');
const {
  getFirestore,
  doc,
  getDoc,
  updateDoc,
  collection,
  getDocs,
} = require('firebase/firestore');
const { pipeline, AutoTokenizer } = require('@xenova/transformers');

// Firebase ì„¤ì •
const firebaseConfig = {
  apiKey: 'AIzaSyAQY-tXbLL-u1MLGDo_keO2HmSnmaAOlF0',
  authDomain: 'memorizewholetext.firebaseapp.com',
  projectId: 'memorizewholetext',
  storageBucket: 'memorizewholetext.appspot.com',
  messagingSenderId: '1017620600279',
  appId: '1:1017620600279:web:1ef89648b5c2d17f56e792',
  measurementId: 'G-HYV1GDPW35',
};

// Firebase ì´ˆê¸°í™”
const app = initializeApp(firebaseConfig);
const db = getFirestore(app);

let transformersExtractorPromise = null;
let tokenizerPromise = null;

async function getTransformersExtractor() {
  if (!transformersExtractorPromise) {
    transformersExtractorPromise = pipeline(
      'feature-extraction',
      'Xenova/all-MiniLM-L6-v2'
    );
  }
  return transformersExtractorPromise;
}

async function getTokenizer() {
  if (!tokenizerPromise) {
    tokenizerPromise = AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2');
  }
  return tokenizerPromise;
}

// Transformers.jsë¡œ embedding ìƒì„±
async function generateTransformersEmbedding(text) {
  try {
    const extractor = await getTransformersExtractor();
    const output = await extractor(text, { pooling: 'mean', normalize: true });
    
    // Tensorë¥¼ ë°°ì—´ë¡œ ë³€í™˜
    let embedding = [];
    if (Array.isArray(output)) {
      embedding = output;
    } else if (output.data) {
      if (Array.isArray(output.data)) {
        embedding = output.data;
      } else if (output.data && typeof output.data === 'object' && 'length' in output.data) {
        embedding = Array.from(output.data);
      }
    } else if (typeof output === 'object' && 'length' in output) {
      embedding = Array.from(output);
    }
    
    return embedding;
  } catch (error) {
    console.error('Transformers.js Embedding ìƒì„± ì˜¤ë¥˜:', error);
    throw error;
  }
}

// íŠ¹ì • ë‹¨ì–´ì˜ í† í° ì„ë² ë”© ìƒì„± (ë¬¸ë§¥ ê¸°ë°˜)
async function generateTokenEmbeddingForWord(text, targetWord) {
  try {
    // ì…ë ¥ì„ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜
    const inputText = (typeof text === 'string')
      ? text
      : (text && typeof text.toString === 'function')
        ? text.toString()
        : String(text ?? '');
    const targetStr = (typeof targetWord === 'string')
      ? targetWord
      : (targetWord && typeof targetWord.toString === 'function')
        ? targetWord.toString()
        : String(targetWord ?? '');

    const extractor = await getTransformersExtractor();
    // í† í°ë³„ íˆë“  ìƒíƒœ ì¶”ì¶œ
    const output = await extractor(inputText, { pooling: 'none', normalize: false });

    // tokenVectorsë¥¼ [tokens, hidden] í˜•íƒœì˜ 2ì°¨ì› ë°°ì—´ë¡œ ì •ê·œí™”
    let tokenVectors = null;
    if (Array.isArray(output) && output.length > 0) {
      // ì´ë¯¸ 2ì°¨ì› ë°°ì—´ì¸ ê²½ìš°
      if (Array.isArray(output[0])) {
        tokenVectors = output;
      } else if (typeof output[0] === 'number') {
        // ë“œë¬¸ ê²½ìš°: 1ì°¨ì›ë§Œ ì˜¨ë‹¤ë©´ ë³€í™˜ ë¶ˆê°€
        console.warn('feature-extraction ê²°ê³¼ê°€ 1ì°¨ì› ë°°ì—´ì…ë‹ˆë‹¤. í† í° ì„ë² ë”© ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.');
        return [];
      }
    } else if (output && typeof output === 'object') {
      // Tensor í˜•íƒœ ì²˜ë¦¬ (dims, data ê¸°ë°˜)
      // ì˜ˆìƒ dims: [seq, hidden] ë˜ëŠ” [1, seq, hidden]
      const dims = Array.isArray(output.dims) ? output.dims : null;
      const data = output.data;
      if (dims && data && (Array.isArray(data) || (data.BYTES_PER_ELEMENT !== undefined))) {
        const flat = Array.isArray(data) ? data : Array.from(data);
        let seq = 0;
        let hidden = 0;
        if (dims.length === 2) {
          seq = dims[0];
          hidden = dims[1];
        } else if (dims.length === 3) {
          const batch = dims[0];
          seq = dims[1];
          hidden = dims[2];
          if (batch !== 1) {
            console.warn(`ì˜ˆìƒì¹˜ ëª»í•œ batch í¬ê¸°: ${batch}. batch=1 ê°€ì •ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.`);
          }
        }
        if (seq > 0 && hidden > 0 && flat.length === seq * hidden) {
          tokenVectors = new Array(seq);
          for (let i = 0; i < seq; i++) {
            const start = i * hidden;
            tokenVectors[i] = flat.slice(start, start + hidden);
          }
        }
      } else if (typeof output.tolist === 'function') {
        const list = output.tolist();
        if (Array.isArray(list) && Array.isArray(list[0])) {
          tokenVectors = list;
        }
      }
    }
    if (!tokenVectors || !Array.isArray(tokenVectors) || tokenVectors.length === 0 || !Array.isArray(tokenVectors[0])) {
      console.warn('feature-extraction ê²°ê³¼ê°€ í† í° ì°¨ì› ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤. í† í° ì„ë² ë”© ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.');
      return [];
    }

    // í† í° ë¬¸ìì—´ ëª©ë¡ (ëª¨ë¸/í† í¬ë‚˜ì´ì €ì— ë”°ë¼ special tokens ì œì™¸ë¨)
    let tokens = [];
    try {
      const tokenizer = await getTokenizer();
      // encode ì„ í˜¸: special tokens ì œì™¸í•˜ì—¬ ì •ë ¬ ìš©ì´
      if (tokenizer && typeof tokenizer.encode === 'function') {
        const enc = await tokenizer.encode(inputText, { add_special_tokens: false });
        if (enc && Array.isArray(enc.tokens) && enc.tokens.length > 0) {
          tokens = enc.tokens;
        }
      }
      if (tokens.length === 0 && tokenizer && typeof tokenizer.tokenize === 'function') {
        tokens = tokenizer.tokenize(inputText) || [];
      } else if (tokenizer && typeof tokenizer.encode === 'function') {
        // ì¼ë¶€ êµ¬í˜„ì—ì„œëŠ” encode ê²°ê³¼ì— tokensê°€ ìˆìŒ
        const enc = await tokenizer.encode(inputText, { add_special_tokens: false });
        if (enc && Array.isArray(enc.tokens)) {
          tokens = enc.tokens;
        }
      }
    } catch (e) {
      console.warn('í† í¬ë‚˜ì´ì € í† í° ì¶”ì¶œ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤:', e?.message || e);
      tokens = (inputText && typeof inputText === 'string') ? (inputText.match(/\S+/g) || []) : [];
    }

    // special token ë³´ì •ì¹˜ ì¶”ì •
    let offset = 0;
    if (tokens.length > 0 && (tokenVectors.length - tokens.length === 2)) {
      // [CLS], [SEP]ê°€ ì¶”ê°€ëœ ì „í˜•ì ì¸ ê²½ìš°
      offset = 1; // tokenVectorsì—ì„œ ì‹¤ì œ ì²« í† í° ìœ„ì¹˜
    }

    const clean = (t) => (t || '').replace(/^##/, '').replace(/^â–/, '').toLowerCase();
    const target = String(targetStr || '').toLowerCase();

    const matchIndices = [];

    if (tokens.length > 0) {
      // 1) ê°„ë‹¨ í¬í•¨/ë™ë“± ë§¤ì¹­
      for (let i = 0; i < tokens.length; i++) {
        const tok = clean(tokens[i]);
        if (!tok) continue;
        if (tok === target || tok.includes(target) || target.includes(tok)) {
          const tvIdx = i + offset;
          if (tvIdx >= 0 && tvIdx < tokenVectors.length) {
            matchIndices.push(tvIdx);
          }
        }
      }

      // 2) ì—°ì† ì„œë¸Œì›Œë“œ ë§¤ì¹­ (fallback)
      if (matchIndices.length === 0) {
        try {
          const tokenizer = await getTokenizer();
          let targetTokens = [];
          if (tokenizer && typeof tokenizer.encode === 'function') {
            const enc = await tokenizer.encode(targetStr, { add_special_tokens: false });
            if (enc && Array.isArray(enc.tokens) && enc.tokens.length > 0) {
              targetTokens = enc.tokens;
            }
          }
          if (targetTokens.length === 0 && tokenizer && typeof tokenizer.tokenize === 'function') {
            targetTokens = tokenizer.tokenize(targetStr) || [];
          }
          const cleanedTargetTokens = targetTokens.map(clean);
          for (let i = 0; i <= tokens.length - cleanedTargetTokens.length; i++) {
            let ok = true;
            for (let j = 0; j < cleanedTargetTokens.length; j++) {
              if (clean(tokens[i + j]) !== cleanedTargetTokens[j]) {
                ok = false;
                break;
              }
            }
            if (ok) {
              for (let j = 0; j < cleanedTargetTokens.length; j++) {
                const tvIdx = i + j + offset;
                if (tvIdx >= 0 && tvIdx < tokenVectors.length) {
                  matchIndices.push(tvIdx);
                }
              }
              break;
            }
          }
        } catch (_) {
          // ignore
        }
      }
    } else {
      // í† í° ë¬¸ìì—´ì„ ì–»ì§€ ëª»í•œ ê²½ìš°, ì „ì²´ í† í°ì„ ìŠ¤ìº”í•˜ë©° ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹± ì ìš© ë¶ˆê°€ â†’ ê±´ë„ˆëœ€
      console.warn('í† í° ë¬¸ìì—´ì„ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í† í° ì„ë² ë”© ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.');
    }

    if (matchIndices.length === 0) {
      // ë§¤ì¹­ ì‹¤íŒ¨
      return [];
    }

    const hiddenSize = tokenVectors[0].length;
    const sum = new Array(hiddenSize).fill(0);
    for (const idx of matchIndices) {
      const vec = tokenVectors[idx];
      for (let d = 0; d < hiddenSize; d++) {
        sum[d] += vec[d];
      }
    }
    // í‰ê·  + L2 ì •ê·œí™”
    const avg = sum.map((v) => v / matchIndices.length);
    const norm = Math.sqrt(avg.reduce((s, v) => s + v * v, 0));
    return norm > 0 ? avg.map((v) => v / norm) : avg;
  } catch (error) {
    console.warn('í† í° ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜:', error?.message || error);
    return [];
  }
}

// TensorFlow.js ìŠ¤íƒ€ì¼ì˜ embedding ìƒì„± (í•´ì‹± ê¸°ë°˜)
// ì°¸ê³ : ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” Universal Sentence Encoderë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
function generateTensorFlowEmbedding(text, embeddingSize) {
  try {
    const words = text.toLowerCase().split(/\s+/);
    const tfEmbedding = new Array(embeddingSize).fill(0);
    
    // ë‹¨ì–´ì˜ ìœ„ì¹˜ì™€ ë¬¸ë§¥ì„ ê³ ë ¤í•œ embedding ìƒì„±
    words.forEach((word, wordIdx) => {
      for (let i = 0; i < word.length; i++) {
        const charCode = word.charCodeAt(i);
        // ë‹¨ì–´ì˜ ìœ„ì¹˜ì™€ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì¸ë±ìŠ¤ ê³„ì‚°
        const pos = (charCode + wordIdx * 100) % embeddingSize;
        // ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ë“œëŸ¬ìš´ ë¶„í¬ ìƒì„±
        tfEmbedding[pos] += Math.sin(charCode * 0.01) * (1.0 / (wordIdx + 1));
      }
    });
    
    // ì •ê·œí™” (L2 norm)
    const norm = Math.sqrt(tfEmbedding.reduce((sum, val) => sum + val * val, 0));
    if (norm > 0) {
      return tfEmbedding.map(val => val / norm);
    }
    
    return tfEmbedding;
  } catch (error) {
    console.error('TensorFlow.js Embedding ìƒì„± ì˜¤ë¥˜:', error);
    throw error;
  }
}

// ë‹¨ì–´ì˜ embedding êµ¬ì¡° ì—…ë°ì´íŠ¸
function hasExistingEmbedding(embedding) {
  if (!embedding) return false;
  if (Array.isArray(embedding)) {
    return embedding.length > 0;
  }
  if (typeof embedding === 'object') {
    const transformers = embedding.transformers;
    const tensorflow = embedding.tensorflow;
    if (Array.isArray(transformers) && transformers.length > 0) return true;
    if (Array.isArray(tensorflow) && tensorflow.length > 0) return true;
  }
  return false;
}

function needsStructureUpgrade(embedding) {
  if (!embedding) return false;
  if (Array.isArray(embedding)) return true;
  if (typeof embedding === 'object') {
    if (!('transformers' in embedding) || !('tensorflow' in embedding)) {
      return true;
    }
  }
  return false;
}

async function updateWordEmbeddings(wordKey) {
  try {
    console.log(`ğŸ”¥ Firebaseì—ì„œ "${wordKey}" ë‹¨ì–´ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...\n`);
    
    // Firebaseì—ì„œ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°
    const wordDocRef = doc(db, 'words', wordKey.toLowerCase());
    const wordDocSnap = await getDoc(wordDocRef);
    
    if (!wordDocSnap.exists()) {
      console.error(`âŒ "${wordKey}" ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
      return;
    }
    
    const wordData = wordDocSnap.data();
    console.log(`ğŸ“„ ë‹¨ì–´: ${wordData.word}`);
    console.log(`ğŸ“Š meanings ê°œìˆ˜: ${wordData.meanings?.length || 0}\n`);
    
    if (!wordData.meanings || wordData.meanings.length === 0) {
      console.log('âš ï¸  meaningsê°€ ì—†ìŠµë‹ˆë‹¤.');
      return;
    }
    
    // Transformers.js ëª¨ë¸ ë¡œë“œ
    console.log('ğŸ¤– Transformers.js ëª¨ë¸ ë¡œë”© ì¤‘...');
    console.log('   (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n');
    
    // ê° meaningì— ëŒ€í•´ embedding ìƒì„±
    const updatedMeanings = [];
    let hasChanges = false;
    
    for (let i = 0; i < wordData.meanings.length; i++) {
      const meaning = wordData.meanings[i];
      console.log(`ğŸ“ ì²˜ë¦¬ ì¤‘: ${i + 1}/${wordData.meanings.length} - ${meaning.id || `meaning_${i}`}`);

      const existingEmbedding = meaning.embedding;
      const hasEmbedding = hasExistingEmbedding(existingEmbedding);
      const upgradeStructure = needsStructureUpgrade(existingEmbedding);

      // examplesì˜ ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ì‚¬ìš© (ì˜ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ)
      let textToEmbed = '';
      if (meaning.examples && meaning.examples.length > 0) {
        const example = meaning.examples[0];
        const englishPart = example.split('(')[0].trim();
        textToEmbed = englishPart.replace(/\*\*/g, '').trim();
      }

      const canGenerate = Boolean(textToEmbed) && !hasEmbedding;

      if (canGenerate) {
        console.log(`   í…ìŠ¤íŠ¸: "${textToEmbed}"`);
        try {
          console.log(`   ğŸ”„ Transformers.js embedding ìƒì„± ì¤‘...`);
          const transformersEmbedding = await generateTransformersEmbedding(textToEmbed);
          console.log(`   âœ… Transformers.js embedding ì™„ë£Œ (ì°¨ì›: ${transformersEmbedding.length})`);

          console.log(`   ğŸ”„ TensorFlow.js embedding ìƒì„± ì¤‘...`);
          const tensorflowEmbedding = generateTensorFlowEmbedding(textToEmbed, transformersEmbedding.length);
          console.log(`   âœ… TensorFlow.js embedding ì™„ë£Œ (ì°¨ì›: ${tensorflowEmbedding.length})`);

          // í† í° ì„ë² ë”© (ë¬¸ë§¥ ë‚´ target ë‹¨ì–´ ê¸°ì¤€)
          let tokenEmbedding = [];
          if (textToEmbed) {
            const targetWord = String(wordData.word || wordKey || '').toLowerCase();
            console.log(`   ğŸ” Token embedding ìƒì„± ëŒ€ìƒ ë‹¨ì–´: "${targetWord}"`);
            tokenEmbedding = await generateTokenEmbeddingForWord(textToEmbed, targetWord);
            if (Array.isArray(tokenEmbedding) && tokenEmbedding.length > 0) {
              console.log(`   âœ… Token embedding ìƒì„± ì™„ë£Œ (ì°¨ì›: ${tokenEmbedding.length})`);
            } else {
              console.log(`   âš ï¸  Token embeddingì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.`);
            }
          }

          updatedMeanings.push({
            ...meaning,
            embedding: {
              transformers: transformersEmbedding,
              tensorflow: tensorflowEmbedding,
              tokenEmbedding: Array.isArray(tokenEmbedding) ? tokenEmbedding : [],
            },
          });
          hasChanges = true;
          console.log(`   âœ… Embedding êµ¬ì¡° ì—…ë°ì´íŠ¸ ì™„ë£Œ\n`);
          continue;
        } catch (error) {
          console.error(`   âŒ Embedding ìƒì„± ì‹¤íŒ¨:`, error.message);
        }
      }

      if (upgradeStructure) {
        let transformers = [];
        let tensorflow = [];
        let tokenEmbedding = [];

        if (Array.isArray(existingEmbedding)) {
          transformers = existingEmbedding;
          console.log('   â„¹ï¸ ê¸°ì¡´ ë°°ì—´ êµ¬ì¡°ë¥¼ ìƒˆ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.');
        } else if (existingEmbedding && typeof existingEmbedding === 'object') {
          transformers = Array.isArray(existingEmbedding.transformers)
            ? existingEmbedding.transformers
            : [];
          tensorflow = Array.isArray(existingEmbedding.tensorflow)
            ? existingEmbedding.tensorflow
            : [];
          tokenEmbedding = Array.isArray(existingEmbedding.tokenEmbedding)
            ? existingEmbedding.tokenEmbedding
            : [];
        }

        // tokenEmbeddingì´ ë¹„ì–´ìˆê³ , ì˜ˆë¬¸ì´ ìˆìœ¼ë©´ ìƒì„± ì‹œë„
        if ((!Array.isArray(tokenEmbedding) || tokenEmbedding.length === 0) && textToEmbed) {
          const targetWord = String(wordData.word || wordKey || '').toLowerCase();
          console.log(`   ğŸ” Token embedding ìƒì„± ëŒ€ìƒ ë‹¨ì–´: "${targetWord}"`);
          const generated = await generateTokenEmbeddingForWord(textToEmbed, targetWord);
          if (Array.isArray(generated) && generated.length > 0) {
            tokenEmbedding = generated;
            console.log(`   âœ… Token embedding ìƒì„± ì™„ë£Œ (ì°¨ì›: ${tokenEmbedding.length})`);
          }
        }

        updatedMeanings.push({
          ...meaning,
          embedding: {
            transformers,
            tensorflow,
            tokenEmbedding: Array.isArray(tokenEmbedding) ? tokenEmbedding : [],
          },
        });
        hasChanges = true;
      } else {
        // ê¸°ì¡´ êµ¬ì¡°ê°€ ì´ë¯¸ ìƒˆ êµ¬ì¡°ì¸ ê²½ìš°ì—ë„ tokenEmbeddingì´ ì—†ìœ¼ë©´ ìƒì„±/ì¶”ê°€
        let nextMeaning = { ...meaning };
        if (!nextMeaning.embedding || typeof nextMeaning.embedding !== 'object') {
          nextMeaning.embedding = { transformers: [], tensorflow: [], tokenEmbedding: [] };
        } else if (!('tokenEmbedding' in nextMeaning.embedding)) {
          nextMeaning.embedding.tokenEmbedding = [];
        }

        const needToken = !Array.isArray(nextMeaning.embedding.tokenEmbedding) || nextMeaning.embedding.tokenEmbedding.length === 0;
        if (needToken && textToEmbed) {
          const targetWord = String(wordData.word || wordKey || '').toLowerCase();
          console.log(`   ğŸ” Token embedding ìƒì„± ëŒ€ìƒ ë‹¨ì–´: "${targetWord}"`);
          const generated = await generateTokenEmbeddingForWord(textToEmbed, targetWord);
          if (Array.isArray(generated) && generated.length > 0) {
            nextMeaning.embedding.tokenEmbedding = generated;
            hasChanges = true;
            console.log(`   âœ… Token embedding ìƒì„± ì™„ë£Œ (ì°¨ì›: ${generated.length})`);
          }
        }

        updatedMeanings.push(nextMeaning);
      }

      if (!canGenerate) {
        if (!textToEmbed) {
          console.log('   âš ï¸  ì˜ˆë¬¸ì´ ì—†ì–´ embeddingì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
        } else if (hasEmbedding) {
          console.log('   âœ… ì´ë¯¸ embeddingì´ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.');
        }
      }

      console.log('');
    }
    
    if (hasChanges) {
      console.log('ğŸ’¾ Firebaseì— ì—…ë°ì´íŠ¸ ì¤‘...');
      await updateDoc(wordDocRef, {
        meanings: updatedMeanings,
        updatedAt: new Date().toISOString(),
      });
      
      console.log('âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n');
      console.log('ğŸ“Š ìš”ì•½:');
      console.log(`   - ì²˜ë¦¬ëœ meanings: ${updatedMeanings.length}ê°œ`);
      const withTransformers = updatedMeanings.filter(
        (m) =>
          m.embedding &&
          m.embedding.transformers &&
          Array.isArray(m.embedding.transformers) &&
          m.embedding.transformers.length > 0
      ).length;
      const withTensorflow = updatedMeanings.filter(
        (m) =>
          m.embedding &&
          m.embedding.tensorflow &&
          Array.isArray(m.embedding.tensorflow) &&
          m.embedding.tensorflow.length > 0
      ).length;
      console.log(`   - Transformers.js embeddingì´ ìˆëŠ” meanings: ${withTransformers}ê°œ`);
      console.log(`   - TensorFlow.js embeddingì´ ìˆëŠ” meanings: ${withTensorflow}ê°œ`);
    } else {
      console.log('â„¹ï¸ ë³€ê²½ ì‚¬í•­ì´ ì—†ì–´ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\n');
    }
    
  } catch (error) {
    console.error('ğŸ’¥ ì˜¤ë¥˜ ë°œìƒ:', error);
    process.exit(1);
  }
}

async function processAllWordsSequentially() {
  try {
    console.log('ğŸ“š words ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n');
    const wordsCol = collection(db, 'words');
    const snapshot = await getDocs(wordsCol);

    console.log(`ğŸ”¢ ì´ ${snapshot.size}ê°œì˜ ë‹¨ì–´ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n`);

    let processed = 0;
    for (const docSnap of snapshot.docs) {
      processed += 1;
      const wordKey = docSnap.id;
      console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
      console.log(`(${processed}/${snapshot.size}) "${wordKey}" ì²˜ë¦¬ ì‹œì‘`);
      await updateWordEmbeddings(wordKey);
    }

    console.log('\nğŸ‰ words ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤!');
  } catch (error) {
    console.error('ğŸ’¥ ì „ì²´ ë‹¨ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
    process.exit(1);
  }
}

// ë©”ì¸ ì‹¤í–‰
async function main() {
  const wordKey = process.argv[2];

  console.log('ğŸš€ Embedding êµ¬ì¡° ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘\n');
  console.log('   ìƒˆë¡œìš´ êµ¬ì¡°:');
  console.log('   {');
  console.log('     "embedding": {');
  console.log('       "transformers": [0.12, -0.23, 0.45, ...],');
  console.log('       "tensorflow": [0.09, 0.17, -0.05, ...]');
  console.log('     }');
  console.log('   }\n');

  if (wordKey) {
    console.log(`ğŸ“‹ "${wordKey}" ë‹¨ì–´ì˜ embeddingì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n`);
    await updateWordEmbeddings(wordKey);
    console.log('ğŸ‰ ì™„ë£Œ!');
  } else {
    await processAllWordsSequentially();
  }
}

main();

