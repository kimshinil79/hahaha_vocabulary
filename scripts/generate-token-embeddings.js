const { initializeApp } = require('firebase/app');
const {
  getFirestore,
  doc,
  getDoc,
  updateDoc,
  collection,
  getDocs,
} = require('firebase/firestore');
const { pipeline, AutoTokenizer } = require('@xenova/transformers');

const firebaseConfig = {
  apiKey: 'AIzaSyAQY-tXbLL-u1MLGDo_keO2HmSnmaAOlF0',
  authDomain: 'memorizewholetext.firebaseapp.com',
  projectId: 'memorizewholetext',
  storageBucket: 'memorizewholetext.appspot.com',
  messagingSenderId: '1017620600279',
  appId: '1:1017620600279:web:1ef89648b5c2d17f56e792',
  measurementId: 'G-HYV1GDPW35',
};

const app = initializeApp(firebaseConfig);
const db = getFirestore(app);

let extractorPromise = null;
let tokenizerPromise = null;

async function getExtractor() {
  if (!extractorPromise) {
    extractorPromise = pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
  }
  return extractorPromise;
}

async function getTokenizer() {
  if (!tokenizerPromise) {
    tokenizerPromise = AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2');
  }
  return tokenizerPromise;
}

async function generateTokenEmbeddingForWord(text, targetWord) {
  try {
    const inputText = typeof text === 'string' ? text : String(text ?? '');
    const target = typeof targetWord === 'string' ? targetWord : String(targetWord ?? '');

    const extractor = await getExtractor();
    const output = await extractor(inputText, { pooling: 'none', normalize: false });

    let tokenVectors = null;
    if (Array.isArray(output) && output.length > 0) {
      if (Array.isArray(output[0])) {
        tokenVectors = output;
      }
    } else if (output && typeof output === 'object') {
      const dims = Array.isArray(output.dims) ? output.dims : null;
      const data = output.data;
      if (dims && data && (Array.isArray(data) || data.BYTES_PER_ELEMENT !== undefined)) {
        const flat = Array.isArray(data) ? data : Array.from(data);
        let seq = 0;
        let hidden = 0;
        if (dims.length === 2) {
          [seq, hidden] = dims;
        } else if (dims.length === 3) {
          seq = dims[1];
          hidden = dims[2];
        }
        if (seq > 0 && hidden > 0 && flat.length === seq * hidden) {
          tokenVectors = new Array(seq);
          for (let i = 0; i < seq; i++) {
            const start = i * hidden;
            tokenVectors[i] = flat.slice(start, start + hidden);
          }
        }
      } else if (typeof output.tolist === 'function') {
        const list = output.tolist();
        if (Array.isArray(list) && Array.isArray(list[0])) {
          tokenVectors = list;
        }
      }
    }

    if (!tokenVectors || !Array.isArray(tokenVectors[0])) {
      console.warn('í† í° ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
      return [];
    }

    let tokens = [];
    try {
      const tokenizer = await getTokenizer();
      if (tokenizer && typeof tokenizer.encode === 'function') {
        const enc = await tokenizer.encode(inputText, { add_special_tokens: false });
        if (enc && Array.isArray(enc.tokens) && enc.tokens.length > 0) {
          tokens = enc.tokens;
        }
      }
      if (!tokens.length && tokenizer && typeof tokenizer.tokenize === 'function') {
        tokens = tokenizer.tokenize(inputText) || [];
      }
    } catch (error) {
      console.warn('í† í° ë¬¸ìì—´ ì¶”ì¶œ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± ì ìš©:', error?.message || error);
      tokens = inputText ? (inputText.match(/\S+/g) || []) : [];
    }

    let offset = 0;
    if (tokens.length && tokenVectors.length - tokens.length === 2) {
      offset = 1;
    }

    const clean = (value) => (value || '').replace(/^##/, '').replace(/^â–/, '').toLowerCase();
    const targetLower = String(target ?? '').toLowerCase();
    const matchIndices = [];

    if (tokens.length) {
      for (let i = 0; i < tokens.length; i++) {
        const tok = clean(tokens[i]);
        if (!tok) continue;
        if (tok === targetLower || tok.includes(targetLower) || targetLower.includes(tok)) {
          const vectorIndex = i + offset;
          if (vectorIndex >= 0 && vectorIndex < tokenVectors.length) {
            matchIndices.push(vectorIndex);
          }
        }
      }

      if (!matchIndices.length) {
        try {
          const tokenizer = await getTokenizer();
          let targetTokens = [];
          if (tokenizer && typeof tokenizer.encode === 'function') {
            const encoded = await tokenizer.encode(target, { add_special_tokens: false });
            if (encoded && Array.isArray(encoded.tokens) && encoded.tokens.length > 0) {
              targetTokens = encoded.tokens;
            }
          }
          if (!targetTokens.length && tokenizer && typeof tokenizer.tokenize === 'function') {
            targetTokens = tokenizer.tokenize(target) || [];
          }
          const cleanedTargetTokens = targetTokens.map(clean);
          for (let i = 0; i <= tokens.length - cleanedTargetTokens.length; i++) {
            let ok = true;
            for (let j = 0; j < cleanedTargetTokens.length; j++) {
              if (clean(tokens[i + j]) !== cleanedTargetTokens[j]) {
                ok = false;
                break;
              }
            }
            if (ok) {
              for (let j = 0; j < cleanedTargetTokens.length; j++) {
                const vectorIndex = i + j + offset;
                if (vectorIndex >= 0 && vectorIndex < tokenVectors.length) {
                  matchIndices.push(vectorIndex);
                }
              }
              break;
            }
          }
        } catch (error) {
          console.warn('ì„œë¸Œì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨:', error?.message || error);
        }
      }
    }

    if (!matchIndices.length) {
      console.warn('ëŒ€ìƒ ë‹¨ì–´ì™€ ë§¤ì¹­ë˜ëŠ” í† í°ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
      return [];
    }

    const hiddenSize = tokenVectors[0].length;
    const sum = new Array(hiddenSize).fill(0);
    for (const idx of matchIndices) {
      const vector = tokenVectors[idx];
      for (let d = 0; d < hiddenSize; d++) {
        sum[d] += vector[d];
      }
    }

    const avg = sum.map((value) => value / matchIndices.length);
    const norm = Math.sqrt(avg.reduce((acc, value) => acc + value * value, 0));
    return norm > 0 ? avg.map((value) => value / norm) : avg;
  } catch (error) {
    console.warn('í† í° ì„ë² ë”© ìƒì„± ì˜¤ë¥˜:', error?.message || error);
    return [];
  }
}

function normalizeExample(example) {
  if (!example || typeof example !== 'string') return '';
  const englishPart = example.split('(')[0] || example;
  return englishPart.replace(/\*\*/g, '').trim();
}

function embeddingsEqual(a = [], b = []) {
  if (!Array.isArray(a) || !Array.isArray(b)) return false;
  if (a.length !== b.length) return false;
  for (let i = 0; i < a.length; i++) {
    if (Math.abs(a[i] - b[i]) > 1e-8) {
      return false;
    }
  }
  return true;
}

async function updateTokenEmbeddingsForWord(wordKey) {
  const wordDocRef = doc(db, 'words', wordKey.toLowerCase());
  const wordDocSnap = await getDoc(wordDocRef);

  if (!wordDocSnap.exists()) {
    console.warn(`ë‹¨ì–´ "${wordKey}" ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
    return;
  }

  const wordData = wordDocSnap.data();
  const word = (wordData.word || wordKey || '').toLowerCase();
  const meanings = Array.isArray(wordData.meanings) ? wordData.meanings : [];

  if (!meanings.length) {
    console.log(`âš ï¸  "${wordKey}" ë¬¸ì„œì— meaningsê°€ ì—†ìŠµë‹ˆë‹¤.`);
    return;
  }

  console.log(`ğŸ“„ ë‹¨ì–´: ${wordData.word || wordKey}`);
  console.log(`   meanings: ${meanings.length}ê°œ`);

  let hasChanges = false;
  const updatedMeanings = [];

  for (let i = 0; i < meanings.length; i++) {
    const meaning = meanings[i];
    const exampleText = normalizeExample(meaning.examples && meaning.examples[0]);
    const newMeaning = { ...meaning };

    if (!newMeaning.embedding || typeof newMeaning.embedding !== 'object') {
      newMeaning.embedding = { transformers: [], tensorflow: [], tokenEmbedding: [] };
    } else if (!('tokenEmbedding' in newMeaning.embedding)) {
      newMeaning.embedding.tokenEmbedding = [];
    }

    if (!exampleText) {
      console.log(`   (${i + 1}/${meanings.length}) ì˜ˆë¬¸ì´ ì—†ì–´ token embeddingì„ ê±´ë„ˆëœë‹ˆë‹¤.`);
      updatedMeanings.push(newMeaning);
      continue;
    }

    console.log(`   (${i + 1}/${meanings.length}) ì˜ˆë¬¸ ê¸°ë°˜ token embedding ìƒì„± ì¤‘...`);
    const tokenEmbedding = await generateTokenEmbeddingForWord(exampleText, word);
    if (tokenEmbedding.length) {
      if (!embeddingsEqual(tokenEmbedding, newMeaning.embedding.tokenEmbedding)) {
        newMeaning.embedding.tokenEmbedding = tokenEmbedding;
        hasChanges = true;
        console.log('      âœ… token embedding ì—…ë°ì´íŠ¸ ì™„ë£Œ');
      } else {
        console.log('      â„¹ï¸ ê¸°ì¡´ token embeddingê³¼ ë™ì¼í•˜ì—¬ ë³€ê²½ ì—†ìŒ');
      }
    } else {
      console.log('      âš ï¸ token embeddingì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
      if (!Array.isArray(newMeaning.embedding.tokenEmbedding)) {
        newMeaning.embedding.tokenEmbedding = [];
        hasChanges = true;
      }
    }

    updatedMeanings.push(newMeaning);
  }

  if (hasChanges) {
    console.log('ğŸ’¾ Firestore ì—…ë°ì´íŠ¸ ì¤‘...');
    await updateDoc(wordDocRef, {
      meanings: updatedMeanings,
      updatedAt: new Date().toISOString(),
    });
    console.log('âœ… ì €ì¥ ì™„ë£Œ\n');
  } else {
    console.log('â„¹ï¸ ë³€ê²½ ì‚¬í•­ì´ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.\n');
  }
}

async function processAllWords() {
  const wordsCol = collection(db, 'words');
  const snapshot = await getDocs(wordsCol);

  console.log(`ğŸ”¢ ì´ ${snapshot.size}ê°œì˜ ë‹¨ì–´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n`);

  let processed = 0;
  for (const docSnap of snapshot.docs) {
    processed += 1;
    const wordKey = docSnap.id;
    console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
    console.log(`(${processed}/${snapshot.size}) "${wordKey}" ì²˜ë¦¬ ì‹œì‘`);
    await updateTokenEmbeddingsForWord(wordKey);
  }

  console.log('\nğŸ‰ ëª¨ë“  ë‹¨ì–´ì˜ token embedding ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.');
}

async function main() {
  const wordKey = process.argv[2];
  console.log('ğŸš€ Token embedding ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘\n');

  if (wordKey) {
    console.log(`ğŸ“˜ ë‹¨ì¼ ë‹¨ì–´ "${wordKey}" ì²˜ë¦¬\n`);
    await updateTokenEmbeddingsForWord(wordKey);
  } else {
    await processAllWords();
  }
}

main().catch((error) => {
  console.error('ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜:', error);
  process.exit(1);
});
